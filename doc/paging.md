## 页表

> 即使只有 512 MB 的内存, 也能让每个进程的内存空间达到 4GB

> OS 有很多经典的概念/机制, 硬件没有一开始就原生支持, 而是随着漫长岁月而发展起来的

分段的问题
* 换入换出粒度问题: 内存碎片化/换入换出的段太大, IO 成本问题
* 物理内存容量问题: 如果物理内存太小, 本身就是无法容纳一个程序, 那么就没法运行了

分页机制
![img:paging](http://img.phanium.top/20230411185125.png)

每加载一个进程, OS 按照进程中各段的起始范围, 在进程自己的 4GB 虚拟空间中寻找可分配的内存段

页表大小问题 (对于 32 位地址空间)
* 页表项数 1M, 页大小 4K, 页表大小 1M * 20 bit = 2.5M (1M * 32 bit = 4M)
* 页表项数 4K, 页大小 1M, 页表大小 4K * 12 bit = 6K (4K * 32 bit = 16K)
* 页表项数 2^m, 页大小 2^(32-m), 页表大小 2^m * m bit (实际上 2^m * 32)
* 逐字节映射: 页大小 1B, 页表项数 2^32, 页表大小 2^32 * 32b = 16GB
* 总的来说, 页越小, 页表项越多, 页表越大

页表转换 (4G)
* 软件逻辑
    * parse 线性地址, 得到页表索引 (取高 20 位, 乘以 4)
    * 访存查页表, 拼凑得到物理地址 (高 20 位拼接 offset)
* 硬件支持: CPU 至少会集成页部件, 线性地址 -> 通过页部件得到物理地址
* 但无论如何, 就算是一级页表, 现在 cache 也需要至少两次访存

一级页表的局限
* 每个页表 4M, 看上去挺好, 但每个进程创建的时候就都得有自己的页表
* 于是: 再小的进程至少也要 4M

## 二级页表

4G 空间 -> 1K 个页表, 每个页表 1K 项, 每页 4K
* 每个页表大小 1K * 4B, 正好占据 1 页
    * 页表可能位于内存的任意位置, 用 10 bit 作为 PD 索引, 得到 20 bit 以 4K 为单位的地址
* 页目录表(PD) 作为页表(PT) 的表, 也正好占一页..
    * 总大小 1K * 4K + 4K = 4M + 4K
    * 最差情况有千分之一的损失....
* 访问过程: 线性地址 -> 通过页部件, 得到页的物理地址 -> 访存, 得到页 -> 通过页部件, 得到物理地址 -> 访存

说到底, 这到底好在哪里了?
* 每个进程都会分配一个页目录表 4K
* 但页目录表上的每个页表可以不立即分配
* 空间上: 最差情况有千分之一的损失.... 但最好情况有: 千分之九百九十九的获益
    * 剩下的都交给统计了
    * 访存的短板: 用 TLB 来弥补

![PD-and-PT](http://img.phanium.top/20230411200433.png)
* Present: 是否在 内存中
* Read/Write: ? (看了 intel 文档, 没错, 页表和段表的机制是有重合的)
    * 其实也没啥奇怪的, 段表我们基本只用到 平坦模型
    * 硬件厂商考虑的要多得多, 段表在我们的 OS 里没有必要性, 但别的 OS 未必就不用
* User/Supervisor
* PWT (Page-level Write-Through)
* PCD (Page-level Cache Disable)
* Accessed
* Dirty
* PAT (Page Attribute Table)
* Global
* Available: OS 来维护, 我猜可能做一些页面置换算法


## 分页, 虚拟地址

### 开启分页

步骤
* 准备好 页目录表 PD 和页表 PT
* 将页目录表地址写入 cr3 (又称 pdbr, 页目录基址寄存器)
* 寄存器 cr0 的 PG 位置 1

CR3, PDBR
![PDBR](http://img.phanium.top/20230411201846.png)
* 页目录表也存在一个 "自然页" 里, 所以只要 20 bit 就能索引到
* 属性只有 PWT PCD
* 如何设置? 简单的 mov cr3, r32

一个简单的问题: 透明访存
* 注意开启分页后的访存总是有 cr3 的参与, 但这件事对我们来说是完全透明的, 借助 cr3 中的物理地址和页部件的配合, 我们能隐式地直接访问 PD,  进一步隐式地直接访问 PT
* 那么能否直接通过 cr3 进行访问呢?
    * 比如 mov ax, [cr3 的高 20 位 << 12]
* 取决于分页开启没开启
    * 如果没开启, 那么访问的就是未来的页表
    * 如果开启了, 那取决于: cr3 作为 vaddr 指向那个 paddr
* 所以 如果你没映射好, 页表就访问不了了

<!-- 删: 傻逼的论证, 事实上, 我的想法太 naive 了, 页表肯定要由 OS 管理, OS 必须能访问到页表, 这个透明是对用户透明, 而不是对 OS 透明 -->

### 页表管理

页表的维护
* 硬件本身提供页表的机制, 之后由 OS 来完善这套机制
    * 硬件的支持本质上来自 软件的需求, 高效率的页表是软硬件协同的结果
* 页表是 local to 进程的, OS 需要做的是动态维护
* 然而在分页后和进程在看待内存上是平等的
    * 关键: 为了管理内存, OS 需要能访问自身的页表

另外还有个问题, 用户程序运行的时候, 经常得用到 OS 提供的服务(用户进程永远无法完整做事)
* 假设每个进程的虚拟地址空间都是独占的, 0-4G 都是自己的, 这样每次 call OS, 应该先切换到 OS, OS 得有自己的地址, 这样换来换取的成本比较大
* 那干脆
    * 让 OS 存在于所有用户进程的虚拟地址里面, 0-3G 是 user, 3G-4G 是 os
    * 让 os 被所有 user "共享"
* 如何实现呢: 只要让进程的 3-4G 的页表项目相同就完事了

看看作者的设计
* PD 位置: paddr 1M 开始
* PD 先将 vaddr 0 和 vaddr 3G 开始的 1M 都映射到 paddr 0-1M
    * (假设 OS kernel 暂时不超过 1M? 前者是临时保留的? 当然 PD 分配的最小粒度是 4M
    * 通过将两者映射到同一个 PT 来实现, 然后 PT 分配前 256 项即可
* 第一个 PT(PT1) 位置: 紧邻 PD 后, paddr 1M + 4K 开始
* vaddr 3G-4G 剩余部分
    * 随后 254 个 PDE, 分配剩余的页表项 (需要 254 个 PT, 直接从 PT1 后面开始分配
    * 最后一个 PDE, 自指 PD: 目的是让 vaddr top 4K 指向 PD

回顾: 分页后, OS 需要拿到 PD 的 vaddr
* 通过分页前的设置: 正常分配就做得到
    * 先指定一个希望映射的 PD 的 vaddr (4K align 的随便某个空闲地址)
    * 分配对应位置的 PDE: 索引为该 vaddr 所在的 4M align, 字段指定一个 PT
    * 然后, PT 取对应偏移处的 PTE, 将其映射到 PD
* 但这里作者没有这么做, 而用了自指的技巧, 更优雅, 简洁

### 魔法与自指

> 破坏约定, 但符合规则

假设: 我们目的是希望通过 top 的 4K vaddr 来访问我们的 PD

回顾下, 正常人的思路是, OS 就该像访存一样访问页表, 那么把 PD vaddr 映射到 PD paddr 就完事了
* 首先, 因为是 top 4K, 必须拿 PD 的最后一项, 随便指向一个页表的地址
* 然后, 这个页表也要映射他的最后一项, 然后指向 PD 的内存地址

这个做法没有任何问题, 我们甚至还空出来 4M-4K, 他们是自由的, 可以随便存别的东西
然而可以不这么想: 既然 PDE 指向的东西就是 4K, 直接用他装一个 PD 不就完了...
看似荒唐, 但如上面所见, 奇迹般地殊途同归
* 首先也是拿 PD 的最后一项, 这里原本能指向一个 (最终能指向 4M 物理地址的)页表, 而魔法师只用来指向一个 PD
* 然后自指发生了, "分页机制"会把这个指向的 PD 当作是 "PT", 然后 "PT" 的最后一项, 被认为是 top 4K vaddr 到 PD paddr 的映射
* 本质在于
    * 注意到, vaddr 本身就已经完全确定了其将来在 PT 和 PD 中的索引的位置
        * PD 的粒度是全局 4M align 的 vaddr 结构, 全局的 top 4K 在全局的 top 4M 中, 所以会访问最后一项
        * PT 的粒度是局部 4K align 的 vaddr 结构, 全局的 top 4K 在局部的 top 4M 中也是 top 4K, 所以会再次访问最后一项
    * 进一步, 又因为 PD 和 PT 的结构恰好一致 (大小, 项数)
    * 那么, 只要设计 vaddr 两部分索引相等, PD 的相应位置指向自己, 就能保证 两次查表的结果都是 PD 自身

当然, 如果你认为: top 4M 的 top 4K 映射恰好是没问题的, 但 4M - 4K (剩下 1023 项) 的映射被浪费了
* 这种做法也不全是优点吗?  或许是, 毕竟这种做法总会浪费掉一个 PDE
* 但实际上: 通过 top 4M - 4K vaddr, 你还能访问其他任意 PT
    * 如果使用普通的做法来映射所有的 PT: 你同样会浪费掉一整个 PDE, 此外还有它实指的 PT
* 这才是自指魔法的强大之处, 既优雅, 又高效

看看魔法的结果吧:

info tab 可显示页表
```python
cr3: 0x000000100000
# 这两项挺正常
0x00000000-0x000fffff -> 0x000000000000-0x0000000fffff
0xc0000000-0xc00fffff -> 0x000000000000-0x0000000fffff
# 这三项看上去很奇怪 但实际确实是正确的
0xffc00000-0xffc00fff -> 0x000000101000-0x000000101fff
0xfff00000-0xffffefff -> 0x000000101000-0x0000001fffff
0xfffff000-0xffffffff -> 0x000000100000-0x000000100fff
```
bochs 当然是是遵守约定的: 总认为每一项 PDE 的内容是一个 PT
* 然而我们是用 PDE 映射了 PD, 而不是 PT
* 根据我们前面的推理, 这一定是正确的结果

不妨人工 parse 一下后面三行
* 首先索引到到 PD 的第 1023 项 (vaddr 最高 10 位全 1), 得到的"页表"还是 PD
* 然后分类讨论 vaddr 中间的 10 位:
    * 如果全 0, 那么就会索引到"页表"的第 0 项, 得到 0x101000 被认为是物理地址, 所以再加上 offset:
        * 实际上物理地址就是第一个页表
    ```
    0xffc00000-0xffc00fff -> 0x000000101000-0x000000101fff
    ```
    * 如果为 11_0000_0000b, 那么就会索引到"页表"的第 768 项, 得到 0x101000
        * 类似 769 -> 0x102000, 直到 1022(11_1111_1110b) -> 0x1ff000
        * 实际上物理地址就是第 769 - 1022 个页表
    ```
    0xfff00000-0xffffefff -> 0x000000101000-0x0000001fffff
    ```
    * 如果为 11_1111_1111b, 那么就会索引到"页表"的第 1023 项, 这一项正好再次指向 PD 自身, 得到 0x100000
        * 实际上物理地址就是页目录自身
    ```
    0xfffff000-0xffffffff -> 0x000000100000-0x000000100fff
    ```

<!-- 勘误: -->
<!-- - 这里原书的第二项映射的不全, 估计是没编译后面添加的 254 个 pde -->

### TLB
translation lokkaside buffer

多级页表代表着多级访存, 要想加速这个过程, 自然就想到缓存, TLB 应运而生

TLB 条目, 例子
* 高二十位虚拟地址 -> 高二十位物理地址
    * 如果 hit, 那么不用访存
    * 如果 miss, 要逐级查页表, 得到物理地址, 并填充到 TLB
    * 如果查页表没找到(P=0), 那么就是缺页中断
* 缓存同步问题: TLB 必须实时更新(依靠 OS 的策略)
* 刷新 TLB
    * 重新加载 cr3
    * 或, 使用 invlpg (invalidate page)
        * `invlpg [0x1234]` 更新 0x1234 对应的条目
        * (而不是 `invlpg 0x1234`)



